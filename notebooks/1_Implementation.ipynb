{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a45fcda",
   "metadata": {},
   "source": [
    "# Lab 02: Softmax Regression - Model Implementation\n",
    "\n",
    "**Group 09:** \n",
    "**Members:**\n",
    "1. Bùi Huy Giáp - 23127289\n",
    "2. Lê Minh Đức - 23127351\n",
    "3. Vũ Tiến Dũng - 23127354\n",
    "4. Đinh Xuân Khương - 23127398\n",
    "5. Nguyễn Đồng Thanh - 23127538 \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4b75e",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "In this notebook, we implement the **Softmax Regression** model from scratch using only `NumPy`. This model is a generalization of Logistic Regression for multi-class classification problems. We will use the MNIST dataset (handwritten digits 0-9) to train and evaluate our model.\n",
    "\n",
    "The goal is to understand the underlying mathematics:\n",
    "1.  **Linear Hypothesis:** $z = Wx + b$\n",
    "2.  **Softmax Activation:** Converting raw scores into probabilities.\n",
    "3.  **Cross-Entropy Loss:** Measuring the difference between predicted probabilities and actual labels.\n",
    "4.  **Gradient Descent:** Updating parameters ($W, b$) to minimize loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80451dfa",
   "metadata": {},
   "source": [
    "## 2. Mathematical Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a5a47",
   "metadata": {},
   "source": [
    "### 2.1. Hypothesis (Mathematical Formulation)\n",
    "\n",
    "For a given input vector $x \\in \\mathbb{R}^{d}$ (where $d$ is the number of features, e.g., $28 \\times 28 = 784$ pixels), and for $K$ classes (here $K=10$), the model computes a linear score (logit) for each class $k$:\n",
    "\n",
    "$$z_k = w_k^T x + b_k$$\n",
    "\n",
    "In vectorized form for a batch of $m$ samples $X \\in \\mathbb{R}^{m \\times d}$, the mathematically rigorous formulation is:\n",
    "\n",
    "$$Z = XW + \\mathbf{1}_m b$$\n",
    "\n",
    "Where:\n",
    "- $W \\in \\mathbb{R}^{d \\times K}$ is the weight matrix.\n",
    "- $b \\in \\mathbb{R}^{1 \\times K}$ is the bias vector.\n",
    "- $\\mathbf{1}_m \\in \\mathbb{R}^{m \\times 1}$ is a column vector of all ones.\n",
    "- $Z \\in \\mathbb{R}^{m \\times K}$ contains the logits.\n",
    "\n",
    "The term $\\mathbf{1}_m b$ performs the operation of replicating the bias vector $b$ for each of the $m$ samples in the batch, resulting in an $(m \\times K)$ matrix that can be validly added to $XW$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa1dccc",
   "metadata": {},
   "source": [
    "### 2.1.1. Note on Implementation (Broadcasting)\n",
    "\n",
    "In practice, when implementing this with Python libraries like **NumPy** (or TensorFlow/PyTorch), we typically omit the $\\mathbf{1}_m$ vector and write the code (and often the documentation) simply as:\n",
    "\n",
    "$$Z = XW + b$$\n",
    "\n",
    "**Why?**\n",
    "This relies on a mechanism called **Broadcasting**. The library automatically detects the dimension mismatch between $XW$ $(m \\times K)$ and $b$ $(1 \\times K)$, and implicitly \"stretches\" (broadcasts) the vector $b$ across all $m$ rows to perform the addition element-wise.\n",
    "\n",
    "Therefore, in our code:\n",
    "```python\n",
    "# Code implementation\n",
    "z = np.dot(X, self.W) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2651aa5d",
   "metadata": {},
   "source": [
    "### 2.2. Softmax Function\n",
    "\n",
    "For a single sample with logit vector $z = [z_1, z_2, \\dots, z_K]^T$, the Softmax function transforms these scores into a probability distribution $\\hat{y}$ (where elements lie in $(0, 1)$ and sum to 1):\n",
    "\n",
    "$$\\hat{y}_k = \\text{Softmax}(z)_k = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
    "\n",
    "When applied to the logits matrix $Z \\in \\mathbb{R}^{m \\times K}$, this operation is performed **row-wise** for each sample.\n",
    "\n",
    "**Numerical Stability:**\n",
    "Directly computing $e^{z_k}$ can lead to floating-point overflow if $z_k$ is large. To prevent this, we utilize the property that Softmax is invariant to constant shifts. We subtract the maximum value in the vector $z$ from each element before exponentiation:\n",
    "\n",
    "$$\\text{Softmax}(z)_k = \\frac{e^{z_k - c}}{\\sum_{j=1}^{K} e^{z_j - c}}$$\n",
    "\n",
    "Where $c = \\max(z)$. This ensures that the largest exponent is $0$ ($e^0=1$), preventing overflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2389f616",
   "metadata": {},
   "source": [
    "### 2.3. Cross-Entropy Loss\n",
    "\n",
    "We optimize the model by minimizing the Cross-Entropy loss function, which measures the dissimilarity between the predicted probability distribution $\\hat{y}$ and the true label distribution $y$.\n",
    "\n",
    "For a single sample with true label $y$ (one-hot encoded) and prediction $\\hat{y}$:\n",
    "\n",
    "$$L(y, \\hat{y}) = - \\sum_{k=1}^{K} y_k \\log(\\hat{y}_k)$$\n",
    "\n",
    "The average loss over a batch of size $m$ is defined as:\n",
    "\n",
    "$$J(W, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y^{(i)}_k \\log(\\hat{y}^{(i)}_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d745141",
   "metadata": {},
   "source": [
    "### 2.3.1. Note on Implementation (Numerical Stability)\n",
    "\n",
    "Mathematically, $\\log(0)$ is undefined (approaches $-\\infty$). In our Python implementation, predicted probabilities $\\hat{y}$ can sometimes be extremely close to 0 due to floating-point limitations.\n",
    "\n",
    "To prevent numerical errors (computational instability), we add a small constant $\\epsilon$ (e.g., $10^{-9}$) inside the logarithm:\n",
    "\n",
    "$$L_{code} = - \\sum_{k=1}^{K} y_k \\log(\\hat{y}_k + \\epsilon)$$\n",
    "\n",
    "This ensures that the term inside the log is always strictly positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63267050",
   "metadata": {},
   "source": [
    "### 2.4. Optimization (Gradient Descent)\n",
    "\n",
    "We calculate the gradients of the cost function with respect to the parameters to minimize loss.\n",
    "\n",
    "First, we calculate the **error term** (gradient with respect to the linear scores $Z$):\n",
    "$$\\delta = \\hat{Y} - Y$$\n",
    "*(Note: Corresponds to `dz` in the code)*\n",
    "\n",
    "Then, using the chain rule, the gradients for weights $W$ and bias $b$ (averaged over $m$ samples) are:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W} = \\frac{1}{m} X^T \\delta$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\delta^{(i)}$$\n",
    "\n",
    "**Standard Update Rule:**\n",
    "$$W := W - \\alpha \\frac{\\partial J}{\\partial W}$$\n",
    "$$b := b - \\alpha \\frac{\\partial J}{\\partial b}$$\n",
    "Where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa38ee0",
   "metadata": {},
   "source": [
    "### 2.4.1. Implementation Note (Advanced Optimizations)\n",
    "\n",
    "In our `SoftmaxRegression` class, we implement two advanced techniques to improve performance and generalization, which modify the standard update rules above:\n",
    "\n",
    "1.  **L2 Regularization (Weight Decay):** To prevent overfitting, we add a penalty term to the gradient of $W$:\n",
    "    $$\\frac{\\partial J_{reg}}{\\partial W} = \\frac{\\partial J}{\\partial W} + \\lambda W$$\n",
    "    *(Where $\\lambda$ is `l2_reg`)*\n",
    "\n",
    "2.  **Momentum:** To accelerate convergence and reduce oscillation, we use a velocity term $v$:\n",
    "    $$v_W = \\mu v_W + \\alpha \\frac{\\partial J_{reg}}{\\partial W}$$\n",
    "    $$W := W - v_W$$\n",
    "    *(Where $\\mu$ is `momentum`)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "145cbbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import urllib.request # Standard library for downloading files\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7303e5b9",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Helper Functions\n",
    "\n",
    "We need to load the MNIST dataset. To ensure efficiency and portability, we will download the dataset in `.npz` (NumPy Zip) format directly from a reliable source (Google Cloud Storage) if it is not already available locally.\n",
    "\n",
    "We implement the following preprocessing steps:\n",
    "1.  **Loading:** Read `x_train`, `y_train`, `x_test`, `y_test` from the `.npz` file.\n",
    "2.  **Normalization:** Scale pixel intensity values from the range $[0, 255]$ to $[0, 1]$ to ensure numerical stability during gradient descent.\n",
    "3.  **Flattening:** Reshape each $28 \\times 28$ image matrix into a flat feature vector of size $784$ ($28 \\times 28 = 784$). This allows us to perform matrix multiplication with our weight matrix $W$.\n",
    "4.  **One-hot encoding:** Convert integer labels (e.g., $y=5$) into binary vectors (e.g., $[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]$) for Cross-Entropy loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5fe4cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/raw/mnist.npz...\n",
      "Original x_train shape: (60000, 28, 28)\n",
      "Processed x_train shape: (60000, 784)\n",
      "Processed x_test shape: (10000, 784)\n",
      "\n",
      "Dataset ready for training:\n",
      "Training set:   X=(55000, 784), y=(55000, 10)\n",
      "Validation set: X=(5000, 784), y=(5000, 10)\n",
      "Test set:       X=(10000, 784), y=(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "def load_mnist_data(data_path='../data/raw/mnist.npz'):\n",
    "    \"\"\"\n",
    "    Downloads and loads the MNIST dataset from a .npz file.\n",
    "    If the file does not exist, it downloads it from Google Cloud.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to save/load the .npz file.\n",
    "        \n",
    "    Returns:\n",
    "        x_train, y_train, x_test, y_test: Numpy arrays.\n",
    "        - x_train: (60000, 784) - Normalized and Flattened\n",
    "        - y_train: (60000,)     - Raw labels\n",
    "        - x_test:  (10000, 784) - Normalized and Flattened\n",
    "        - y_test:  (10000,)     - Raw labels\n",
    "    \"\"\"\n",
    "    # 1. Download if not exists\n",
    "    url = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\"\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    directory = os.path.dirname(data_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Downloading MNIST data from {url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, data_path)\n",
    "            print(\"Download complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download failed: {e}\")\n",
    "            return None, None, None, None\n",
    "    else:\n",
    "        print(f\"Loading data from {data_path}...\")\n",
    "\n",
    "    # 2. Load data using numpy\n",
    "    with np.load(data_path) as f:\n",
    "        x_train, y_train = f['x_train'], f['y_train']\n",
    "        x_test, y_test = f['x_test'], f['y_test']\n",
    "\n",
    "    print(f\"Original x_train shape: {x_train.shape}\") # Expected: (60000, 28, 28)\n",
    "    \n",
    "    # 3. Preprocessing: Normalize and Flatten\n",
    "    # Scale pixel values to [0, 1] (float32)\n",
    "    x_train = x_train.astype(np.float32) / 255.0\n",
    "    x_test = x_test.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Flatten images: (N, 28, 28) -> (N, 784)\n",
    "    # This is CRITICAL for matrix multiplication: Z = XW + b\n",
    "    x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "    \n",
    "    print(f\"Processed x_train shape: {x_train.shape}\") # Expected: (60000, 784)\n",
    "    print(f\"Processed x_test shape: {x_test.shape}\")   # Expected: (10000, 784)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def one_hot_encode(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts integer labels to one-hot encoded vectors.\n",
    "    e.g., 5 -> [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    \n",
    "    Args:\n",
    "        y (numpy.ndarray): Array of integer labels (m,).\n",
    "        num_classes (int): Number of classes.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: One-hot encoded matrix (m, num_classes).\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    one_hot = np.zeros((m, num_classes))\n",
    "    for i in range(m):\n",
    "        one_hot[i, y[i]] = 1\n",
    "    return one_hot\n",
    "\n",
    "# --- EXECUTE LOADING ---\n",
    "try:\n",
    "    # Load data\n",
    "    X_train_full, y_train_full, X_test, y_test = load_mnist_data()\n",
    "    \n",
    "    if X_train_full is not None:\n",
    "        # Create a Validation set from Train set (last 5000 samples)\n",
    "        # We split to tune hyperparameters without touching the Test set\n",
    "        val_size = 5000\n",
    "        X_val = X_train_full[-val_size:]\n",
    "        y_val = y_train_full[-val_size:]\n",
    "        \n",
    "        X_train = X_train_full[:-val_size]\n",
    "        y_train = y_train_full[:-val_size]\n",
    "        \n",
    "        # One-hot encode labels for training\n",
    "        y_train_enc = one_hot_encode(y_train)\n",
    "        y_val_enc = one_hot_encode(y_val)\n",
    "        y_test_enc = one_hot_encode(y_test)\n",
    "        \n",
    "        print(\"\\nDataset ready for training:\")\n",
    "        print(f\"Training set:   X={X_train.shape}, y={y_train_enc.shape}\")\n",
    "        print(f\"Validation set: X={X_val.shape}, y={y_val_enc.shape}\")\n",
    "        print(f\"Test set:       X={X_test.shape}, y={y_test_enc.shape}\")\n",
    "    else:\n",
    "        print(\"Failed to load dataset.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8637b25f",
   "metadata": {},
   "source": [
    "## 4. Softmax Regression Implementation\n",
    "\n",
    "This is the core component of the project. The class encapsulates the mathematical logic derived in Section 2, enhanced with advanced optimization techniques.\n",
    "\n",
    "**Key Implementation Details:**\n",
    "* **Numerical Stability:** We implement `softmax` by subtracting the maximum value from logits to avoid floating-point overflow ($e^{z_k} \\to \\infty$).\n",
    "* **Vectorization:** We use NumPy matrix operations (dot products) instead of explicit `for` loops for efficient computation.\n",
    "* **Mini-batch Gradient Descent:** We update weights after processing a small batch of samples (e.g., 256) rather than the entire dataset. This balances computational efficiency and convergence stability.\n",
    "* **Momentum:** We incorporate a momentum term ($\\mu$) to accelerate Gradient Descent in the relevant direction and dampen oscillations.\n",
    "* **L2 Regularization:** We add a penalty term $\\frac{\\lambda}{2} ||W||^2$ to the loss function to prevent overfitting and improve generalization on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237072ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SoftmaxRegression:\n",
    "#     def __init__(self, n_features, n_classes, learning_rate=0.01):\n",
    "#         \"\"\"\n",
    "#         Initializes the Softmax Regression model parameters.\n",
    "\n",
    "#         Args:\n",
    "#             n_features (int): Number of input features (e.g., 784 for MNIST).\n",
    "#             n_classes (int): Number of output classes (e.g., 10 for MNIST).\n",
    "#             learning_rate (float): Step size for Gradient Descent optimization.\n",
    "#         \"\"\"\n",
    "#         self.n_features = n_features\n",
    "#         self.n_classes = n_classes\n",
    "#         self.lr = learning_rate\n",
    "#         self.losses = []\n",
    "        \n",
    "#         # Initialize weights and bias\n",
    "#         # W: (n_features, n_classes) - Initialized with small random values\n",
    "#         self.W = np.random.randn(n_features, n_classes) * 0.01\n",
    "        \n",
    "#         # b: (1, n_classes) - Initialized with zeros\n",
    "#         self.b = np.zeros((1, n_classes))\n",
    "\n",
    "#     def softmax(self, z):\n",
    "#         \"\"\"\n",
    "#         Computes the softmax activation function with numerical stability.\n",
    "#         Formula: exp(z_i) / sum(exp(z_j))\n",
    "\n",
    "#         Args:\n",
    "#             z (numpy.ndarray): Linear logits (batch_size, n_classes).\n",
    "\n",
    "#         Returns:\n",
    "#             numpy.ndarray: Probabilities (batch_size, n_classes).\n",
    "#         \"\"\"\n",
    "#         # Subtract max value to prevent overflow (Numerical Stability)\n",
    "#         z_stable = z - np.max(z, axis=1, keepdims=True)\n",
    "#         exp_z = np.exp(z_stable)\n",
    "#         return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         \"\"\"\n",
    "#         Performs the forward pass to compute predictions.\n",
    "        \n",
    "#         Args:\n",
    "#             X (numpy.ndarray): Input data (batch_size, n_features).\n",
    "\n",
    "#         Returns:\n",
    "#             numpy.ndarray: Predicted probabilities.\n",
    "#         \"\"\"\n",
    "#         # Linear transformation: Z = XW + b\n",
    "#         z = np.dot(X, self.W) + self.b\n",
    "#         # Activation\n",
    "#         return self.softmax(z)\n",
    "\n",
    "#     def compute_loss(self, y_true, y_pred):\n",
    "#         \"\"\"\n",
    "#         Computes Cross-Entropy Loss.\n",
    "#         L = - sum(y_true * log(y_pred))\n",
    "\n",
    "#         Args:\n",
    "#             y_true (numpy.ndarray): One-hot encoded ground truth.\n",
    "#             y_pred (numpy.ndarray): Predicted probabilities.\n",
    "\n",
    "#         Returns:\n",
    "#             float: Average loss over the batch.\n",
    "#         \"\"\"\n",
    "#         m = y_true.shape[0]\n",
    "#         # Add a small epsilon to avoid log(0) error\n",
    "#         epsilon = 1e-9\n",
    "#         loss = -np.sum(y_true * np.log(y_pred + epsilon)) / m\n",
    "#         return loss\n",
    "\n",
    "#     def backward(self, X, y_true, y_pred):\n",
    "#         \"\"\"\n",
    "#         Computes gradients of the loss with respect to W and b.\n",
    "\n",
    "#         Args:\n",
    "#             X (numpy.ndarray): Input data batch.\n",
    "#             y_true (numpy.ndarray): One-hot encoded ground truth.\n",
    "#             y_pred (numpy.ndarray): Predicted probabilities.\n",
    "\n",
    "#         Returns:\n",
    "#             dw, db: Gradients for weights and bias.\n",
    "#         \"\"\"\n",
    "#         m = X.shape[0]\n",
    "        \n",
    "#         # Gradient of loss w.r.t Z (logits) is simply (Prediction - Truth)\n",
    "#         dz = y_pred - y_true\n",
    "        \n",
    "#         # Gradients w.r.t parameters\n",
    "#         # dW = (1/m) * X.T . dZ\n",
    "#         dw = np.dot(X.T, dz) / m\n",
    "#         # db = (1/m) * sum(dZ)\n",
    "#         db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "        \n",
    "#         return dw, db\n",
    "\n",
    "#     def fit(self, X, y, epochs=100, batch_size=256, verbose=True):\n",
    "#         \"\"\"\n",
    "#         Trains the model using Mini-batch Gradient Descent.\n",
    "\n",
    "#         Args:\n",
    "#             X (numpy.ndarray): Training features (m, n_features).\n",
    "#             y (numpy.ndarray): Training labels One-hot (m, n_classes).\n",
    "#             epochs (int): Number of passes over the entire dataset.\n",
    "#             batch_size (int): Number of samples per gradient update.\n",
    "#         \"\"\"\n",
    "#         m = X.shape[0]\n",
    "#         self.losses = [] # Reset history\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             # Shuffle data at the start of each epoch to ensure randomness\n",
    "#             indices = np.arange(m)\n",
    "#             np.random.shuffle(indices)\n",
    "#             X_shuffled = X[indices]\n",
    "#             y_shuffled = y[indices]\n",
    "\n",
    "#             epoch_loss = 0\n",
    "#             num_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "#             for i in range(num_batches):\n",
    "#                 start_idx = i * batch_size\n",
    "#                 end_idx = min(start_idx + batch_size, m)\n",
    "                \n",
    "#                 X_batch = X_shuffled[start_idx:end_idx]\n",
    "#                 y_batch = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "#                 # 1. Forward pass\n",
    "#                 y_pred = self.forward(X_batch)\n",
    "\n",
    "#                 # 2. Compute Loss (accumulate for reporting)\n",
    "#                 loss = self.compute_loss(y_batch, y_pred)\n",
    "#                 epoch_loss += loss * (end_idx - start_idx) \n",
    "\n",
    "#                 # 3. Backward pass\n",
    "#                 dw, db = self.backward(X_batch, y_batch, y_pred)\n",
    "\n",
    "#                 # 4. Update parameters\n",
    "#                 self.W -= self.lr * dw\n",
    "#                 self.b -= self.lr * db\n",
    "            \n",
    "#             # Average loss for the epoch\n",
    "#             avg_loss = epoch_loss / m\n",
    "#             self.losses.append(avg_loss)\n",
    "            \n",
    "#             if verbose and (epoch + 1) % 10 == 0:\n",
    "#                 print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"\n",
    "#         Predicts class labels for input data.\n",
    "\n",
    "#         Args:\n",
    "#             X (numpy.ndarray): Input data (N, n_features).\n",
    "\n",
    "#         Returns:\n",
    "#             numpy.ndarray: Predicted class indices (N,).\n",
    "#         \"\"\"\n",
    "#         y_pred_probs = self.forward(X)\n",
    "#         return np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "#     def save_weights(self, filepath):\n",
    "#         \"\"\"Saves model weights to a .npz file.\"\"\"\n",
    "#         # Ensure directory exists\n",
    "#         directory = os.path.dirname(filepath)\n",
    "#         if directory and not os.path.exists(directory):\n",
    "#             os.makedirs(directory)\n",
    "            \n",
    "#         np.savez(filepath, W=self.W, b=self.b)\n",
    "#         print(f\"Model saved to {filepath}\")\n",
    "\n",
    "#     def load_weights(self, filepath):\n",
    "#         \"\"\"Loads model weights from a .npz file.\"\"\"\n",
    "#         if not os.path.exists(filepath):\n",
    "#             print(f\"File not found: {filepath}\")\n",
    "#             return\n",
    "            \n",
    "#         data = np.load(filepath)\n",
    "#         self.W = data['W']\n",
    "#         self.b = data['b']\n",
    "#         print(f\"Model loaded from {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df8c8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    def __init__(self, n_features, n_classes, learning_rate=0.1, momentum=0.9, l2_reg=1e-4):\n",
    "        \"\"\"\n",
    "        Softmax Regression with Momentum and L2 Regularization.\n",
    "\n",
    "        Args:\n",
    "            n_features (int): Number of input features (e.g., 784).\n",
    "            n_classes (int): Number of output classes (e.g., 10).\n",
    "            learning_rate (float): Step size for parameter updates.\n",
    "            momentum (float): Momentum factor (0.0 to 1.0) to accelerate convergence.\n",
    "            l2_reg (float): L2 regularization strength (lambda) to prevent overfitting.\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.l2_reg = l2_reg\n",
    "        self.losses = []\n",
    "        \n",
    "        # Initialize weights (W) and bias (b)\n",
    "        # Using Xavier/He initialization concept (small random numbers)\n",
    "        self.W = np.random.randn(n_features, n_classes) * 0.01\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "        \n",
    "        # Velocity terms for Momentum\n",
    "        self.v_W = np.zeros_like(self.W)\n",
    "        self.v_b = np.zeros_like(self.b)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        Computes the stable softmax of vector z.\n",
    "        Args:\n",
    "            z (numpy.ndarray): Logits (batch_size, n_classes).\n",
    "        Returns:\n",
    "            numpy.ndarray: Probabilities summing to 1 for each row.\n",
    "        \"\"\"\n",
    "        # Numerical stability: subtract max(z) to prevent exponential overflow\n",
    "        z_stable = z - np.max(z, axis=1, keepdims=True)\n",
    "        exp_z = np.exp(z_stable)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs forward pass: Z = XW + b -> Softmax(Z).\n",
    "        \"\"\"\n",
    "        z = np.dot(X, self.W) + self.b\n",
    "        return self.softmax(z)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes Cross-Entropy Loss with L2 Regularization penalty.\n",
    "        Cost = -Sum(y * log(y_hat)) + (lambda/2) * ||W||^2\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        epsilon = 1e-9 # Prevent log(0)\n",
    "        \n",
    "        # Standard Cross-Entropy\n",
    "        data_loss = -np.sum(y_true * np.log(y_pred + epsilon)) / m\n",
    "        \n",
    "        # L2 Regularization Term\n",
    "        reg_loss = (self.l2_reg / 2) * np.sum(np.square(self.W))\n",
    "        \n",
    "        return data_loss + reg_loss\n",
    "\n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes gradients with respect to W and b (including L2 term).\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        dz = y_pred - y_true\n",
    "        \n",
    "        # Gradient of W: (1/m) * X.T.dot(dZ) + lambda * W\n",
    "        dw = (np.dot(X.T, dz) / m) + (self.l2_reg * self.W)\n",
    "        \n",
    "        # Gradient of b: (1/m) * sum(dZ)\n",
    "        db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return dw, db\n",
    "\n",
    "    def fit(self, X, y, epochs=100, batch_size=256, verbose=True):\n",
    "        \"\"\"\n",
    "        Trains the model using Mini-batch Gradient Descent with Momentum.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        self.losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.arange(m)\n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            epoch_loss = 0\n",
    "            num_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "            for i in range(num_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min(start_idx + batch_size, m)\n",
    "                \n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "                # 1. Forward\n",
    "                y_pred = self.forward(X_batch)\n",
    "\n",
    "                # 2. Loss\n",
    "                loss = self.compute_loss(y_batch, y_pred)\n",
    "                epoch_loss += loss * (end_idx - start_idx)\n",
    "\n",
    "                # 3. Backward\n",
    "                dw, db = self.backward(X_batch, y_batch, y_pred)\n",
    "\n",
    "                # 4. Update with Momentum\n",
    "                # v = momentum * v - learning_rate * gradient\n",
    "                # W = W + v\n",
    "                # Note: We implement standard SGD update logic: W = W - (lr*grad + mom*v_prev)\n",
    "                # Here uses simple Polyak Momentum\n",
    "                self.v_W = self.momentum * self.v_W + self.lr * dw\n",
    "                self.v_b = self.momentum * self.v_b + self.lr * db\n",
    "                \n",
    "                self.W -= self.v_W\n",
    "                self.b -= self.v_b\n",
    "            \n",
    "            avg_loss = epoch_loss / m\n",
    "            self.losses.append(avg_loss)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Returns class indices with highest probability.\"\"\"\n",
    "        y_pred_probs = self.forward(X)\n",
    "        return np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "    def save_weights(self, filepath):\n",
    "        \"\"\"Saves W and b to .npz file.\"\"\"\n",
    "        directory = os.path.dirname(filepath)\n",
    "        if directory and not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        np.savez(filepath, W=self.W, b=self.b)\n",
    "        print(f\"Weights saved to {filepath}\")\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        \"\"\"Loads W and b from .npz file.\"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"File {filepath} not found.\")\n",
    "            return False\n",
    "        data = np.load(filepath)\n",
    "        self.W = data['W']\n",
    "        self.b = data['b']\n",
    "        print(f\"Weights loaded from {filepath}\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c6684",
   "metadata": {},
   "source": [
    "## 5. Sanity Check (Overfitting Test)\n",
    "\n",
    "Before training on the full 55,000 images, we perform a **Sanity Check**. \n",
    "We take a tiny subset of real data (e.g., 100 images) and train the model for many epochs. \n",
    "\n",
    "* **Goal:** The model should be able to memorize this small dataset perfectly (Loss $\\to$ 0, Accuracy $\\to$ 100%).\n",
    "* **Result:** If the model fails to overfit this small batch, there is a bug in the code (likely in Gradient Descent or Backpropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dd864cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check subset: (100, 784)\n",
      "\n",
      "Starting Sanity Check...\n",
      "Epoch 10/200 - Loss: 0.0114\n",
      "Epoch 20/200 - Loss: 0.0086\n",
      "Epoch 30/200 - Loss: 0.0080\n",
      "Epoch 40/200 - Loss: 0.0075\n",
      "Epoch 50/200 - Loss: 0.0072\n",
      "Epoch 60/200 - Loss: 0.0070\n",
      "Epoch 70/200 - Loss: 0.0068\n",
      "Epoch 80/200 - Loss: 0.0067\n",
      "Epoch 90/200 - Loss: 0.0066\n",
      "Epoch 100/200 - Loss: 0.0065\n",
      "Epoch 110/200 - Loss: 0.0064\n",
      "Epoch 120/200 - Loss: 0.0063\n",
      "Epoch 130/200 - Loss: 0.0063\n",
      "Epoch 140/200 - Loss: 0.0062\n",
      "Epoch 150/200 - Loss: 0.0062\n",
      "Epoch 160/200 - Loss: 0.0061\n",
      "Epoch 170/200 - Loss: 0.0061\n",
      "Epoch 180/200 - Loss: 0.0061\n",
      "Epoch 190/200 - Loss: 0.0061\n",
      "Epoch 200/200 - Loss: 0.0060\n",
      "\n",
      "Sanity Check Accuracy: 100.00%\n",
      "✅ Sanity Check PASSED: Model logic is correct.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Select a small subset of REAL data\n",
    "subset_size = 100\n",
    "X_subset = X_train[:subset_size]\n",
    "y_subset_enc = y_train_enc[:subset_size]\n",
    "y_subset_labels = y_train[:subset_size] # Original labels for accuracy check\n",
    "\n",
    "print(f\"Sanity check subset: {X_subset.shape}\")\n",
    "\n",
    "# 2. Initialize Model\n",
    "# 784 features (pixels), 10 classes (digits 0-9)\n",
    "model_test = SoftmaxRegression(n_features=784, n_classes=10, learning_rate=0.1)\n",
    "\n",
    "# 3. Train on subset (Overfitting)\n",
    "print(\"\\nStarting Sanity Check...\")\n",
    "model_test.fit(X_subset, y_subset_enc, epochs=200, batch_size=20, verbose=True)\n",
    "\n",
    "# 4. Check Prediction Accuracy\n",
    "preds = model_test.predict(X_subset)\n",
    "acc = np.mean(preds == y_subset_labels)\n",
    "print(f\"\\nSanity Check Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "# 5. Evaluate\n",
    "if acc > 0.95:\n",
    "    print(\"✅ Sanity Check PASSED: Model logic is correct.\")\n",
    "else:\n",
    "    print(\"❌ Sanity Check FAILED: Model cannot learn even small data.\")\n",
    "\n",
    "# 6. Visualize Loss Curve\n",
    "plt.plot(model_test.losses)\n",
    "plt.title(\"Sanity Check Loss Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4845011",
   "metadata": {},
   "source": [
    "### 5.1. Sanity Check Analysis & Discussion\n",
    "\n",
    "**Observation:**\n",
    "As shown in the loss curve above, the training loss decreased significantly from an initial high value to near zero. The model achieved **100% accuracy** (or very close to it) on the subset of 100 samples.\n",
    "\n",
    "**Discussion on Overfitting:**\n",
    "We observe that the model has successfully **overfitted** this small subset. In the context of a Sanity Check, this is a **positive outcome** because:\n",
    "1.  **Verification of Code Logic:** It confirms that the Forward pass, Backward pass (Gradient calculation), and Update rules are implemented correctly without bugs. If the code were broken, the model would fail to converge even on this tiny dataset.\n",
    "2.  **Model Capacity:** It demonstrates that our Linear Model (Softmax Regression) has sufficient capacity to memorize the mapping of inputs to labels for a small data sample.\n",
    "\n",
    "**Conclusion:**\n",
    "The core implementation is mathematically correct. We can now proceed to training on the full dataset, where our goal will shift from *memorization* (overfitting) to *generalization* (performing well on unseen Test data)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
