{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a45fcda",
   "metadata": {},
   "source": [
    "# Lab 02: Softmax Regression - Model Implementation\n",
    "\n",
    "**Group 09:** \n",
    "**Members:**\n",
    "1. Bùi Huy Giáp - 23127289\n",
    "2. Lê Minh Đức - 23127351\n",
    "3. Vũ Tiến Dũng - 23127354\n",
    "4. Đinh Xuân Khương - 23127398\n",
    "5. Nguyễn Đồng Thanh - 23127538 \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4b75e",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "In this notebook, we implement the **Softmax Regression** model from scratch using only `NumPy`. This model is a generalization of Logistic Regression for multi-class classification problems. We will use the MNIST dataset (handwritten digits 0-9) to train and evaluate our model.\n",
    "\n",
    "The goal is to understand the underlying mathematics:\n",
    "1.  **Linear Hypothesis:** $z = Wx + b$\n",
    "2.  **Softmax Activation:** Converting raw scores into probabilities.\n",
    "3.  **Cross-Entropy Loss:** Measuring the difference between predicted probabilities and actual labels.\n",
    "4.  **Gradient Descent:** Updating parameters ($W, b$) to minimize loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80451dfa",
   "metadata": {},
   "source": [
    "## 2. Mathematical Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a5a47",
   "metadata": {},
   "source": [
    "### 2.1. Hypothesis\n",
    "For a given input vector $x \\in \\mathbb{R}^{d}$ (where $d$ is the number of features, e.g., $28 \\times 28 = 784$ pixels), and for $K$ classes (here $K=10$), the model computes a linear score (logit) for each class $k$:\n",
    "\n",
    "$$z_k = w_k^T x + b_k$$\n",
    "\n",
    "In vectorized form for a batch of $m$ samples $X \\in \\mathbb{R}^{m \\times d}$:\n",
    "$$Z = XW + b$$\n",
    "Where:\n",
    "- $W \\in \\mathbb{R}^{d \\times K}$ is the weight matrix.\n",
    "- $b \\in \\mathbb{R}^{1 \\times K}$ is the bias vector.\n",
    "- $Z \\in \\mathbb{R}^{m \\times K}$ contains the logits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2651aa5d",
   "metadata": {},
   "source": [
    "### 2.2. Softmax Function\n",
    "To transform logits $Z$ into valid probabilities $\\hat{Y}$ (where elements sum to 1), we use the Softmax function:\n",
    "\n",
    "$$\\hat{y}_k = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
    "\n",
    "**Numerical Stability:**\n",
    "Directly computing $e^{z_k}$ can lead to overflow if $z_k$ is large. To prevent this, we subtract the maximum logit from $Z$ before exponentiation:\n",
    "$$\\text{Softmax}(z)_k = \\frac{e^{z_k - \\max(z)}}{\\sum_{j=1}^{K} e^{z_j - \\max(z)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2389f616",
   "metadata": {},
   "source": [
    "### 2.3. Cross-Entropy Loss\n",
    "We optimize the model by minimizing the Cross-Entropy loss. For a single sample with true label $y$ (one-hot encoded) and prediction $\\hat{y}$:\n",
    "\n",
    "$$L(y, \\hat{y}) = - \\sum_{k=1}^{K} y_k \\log(\\hat{y}_k)$$\n",
    "\n",
    "Average loss over batch of size $m$:\n",
    "$$J(W, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y^{(i)}_k \\log(\\hat{y}^{(i)}_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63267050",
   "metadata": {},
   "source": [
    "### 2.4. Optimization (Gradient Descent)\n",
    "We compute the gradients of the loss function with respect to weights $W$ and bias $b$.\n",
    "Using the chain rule, the gradient of the loss with respect to logits $Z$ is:\n",
    "$$\\frac{\\partial J}{\\partial Z} = \\hat{Y} - Y$$\n",
    "\n",
    "Thus, the gradients for parameters are:\n",
    "$$\\frac{\\partial J}{\\partial W} = \\frac{1}{m} X^T (\\hat{Y} - Y)$$\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})$$\n",
    "\n",
    "Update rule:\n",
    "$$W := W - \\alpha \\frac{\\partial J}{\\partial W}$$\n",
    "$$b := b - \\alpha \\frac{\\partial J}{\\partial b}$$\n",
    "Where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145cbbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import urllib.request # Standard library for downloading files\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7303e5b9",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Helper Functions\n",
    "\n",
    "We need to load the MNIST dataset. To ensure efficiency and portability, we will download the dataset in `.npz` (NumPy Zip) format directly from a reliable source (Google Cloud Storage) if it is not already available locally.\n",
    "\n",
    "We implement the following preprocessing steps:\n",
    "1.  **Loading:** Read `x_train`, `y_train`, `x_test`, `y_test` from the `.npz` file.\n",
    "2.  **Normalization:** Scale pixel intensity values from the range $[0, 255]$ to $[0, 1]$ to ensure numerical stability during gradient descent.\n",
    "3.  **Flattening:** Reshape each $28 \\times 28$ image matrix into a flat feature vector of size $784$ ($28 \\times 28 = 784$). This allows us to perform matrix multiplication with our weight matrix $W$.\n",
    "4.  **One-hot encoding:** Convert integer labels (e.g., $y=5$) into binary vectors (e.g., $[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]$) for Cross-Entropy loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5fe4cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading MNIST data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz...\n",
      "Download complete.\n",
      "Original x_train shape: (60000, 28, 28)\n",
      "Processed x_train shape: (60000, 784)\n",
      "Processed x_test shape: (10000, 784)\n",
      "\n",
      "Dataset ready for training:\n",
      "Training set:   X=(55000, 784), y=(55000, 10)\n",
      "Validation set: X=(5000, 784), y=(5000, 10)\n",
      "Test set:       X=(10000, 784), y=(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "def load_mnist_data(data_path='../data/raw/mnist.npz'):\n",
    "    \"\"\"\n",
    "    Downloads and loads the MNIST dataset from a .npz file.\n",
    "    If the file does not exist, it downloads it from Google Cloud.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to save/load the .npz file.\n",
    "        \n",
    "    Returns:\n",
    "        x_train, y_train, x_test, y_test: Numpy arrays.\n",
    "        - x_train: (60000, 784) - Normalized and Flattened\n",
    "        - y_train: (60000,)     - Raw labels\n",
    "        - x_test:  (10000, 784) - Normalized and Flattened\n",
    "        - y_test:  (10000,)     - Raw labels\n",
    "    \"\"\"\n",
    "    # 1. Download if not exists\n",
    "    url = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\"\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    directory = os.path.dirname(data_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Downloading MNIST data from {url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, data_path)\n",
    "            print(\"Download complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download failed: {e}\")\n",
    "            return None, None, None, None\n",
    "    else:\n",
    "        print(f\"Loading data from {data_path}...\")\n",
    "\n",
    "    # 2. Load data using numpy\n",
    "    with np.load(data_path) as f:\n",
    "        x_train, y_train = f['x_train'], f['y_train']\n",
    "        x_test, y_test = f['x_test'], f['y_test']\n",
    "\n",
    "    print(f\"Original x_train shape: {x_train.shape}\") # Expected: (60000, 28, 28)\n",
    "    \n",
    "    # 3. Preprocessing: Normalize and Flatten\n",
    "    # Scale pixel values to [0, 1] (float32)\n",
    "    x_train = x_train.astype(np.float32) / 255.0\n",
    "    x_test = x_test.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Flatten images: (N, 28, 28) -> (N, 784)\n",
    "    # This is CRITICAL for matrix multiplication: Z = XW + b\n",
    "    x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "    \n",
    "    print(f\"Processed x_train shape: {x_train.shape}\") # Expected: (60000, 784)\n",
    "    print(f\"Processed x_test shape: {x_test.shape}\")   # Expected: (10000, 784)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def one_hot_encode(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts integer labels to one-hot encoded vectors.\n",
    "    e.g., 5 -> [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    \n",
    "    Args:\n",
    "        y (numpy.ndarray): Array of integer labels (m,).\n",
    "        num_classes (int): Number of classes.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: One-hot encoded matrix (m, num_classes).\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    one_hot = np.zeros((m, num_classes))\n",
    "    for i in range(m):\n",
    "        one_hot[i, y[i]] = 1\n",
    "    return one_hot\n",
    "\n",
    "# --- EXECUTE LOADING ---\n",
    "try:\n",
    "    # Load data\n",
    "    X_train_full, y_train_full, X_test, y_test = load_mnist_data()\n",
    "    \n",
    "    if X_train_full is not None:\n",
    "        # Create a Validation set from Train set (last 5000 samples)\n",
    "        # We split to tune hyperparameters without touching the Test set\n",
    "        val_size = 5000\n",
    "        X_val = X_train_full[-val_size:]\n",
    "        y_val = y_train_full[-val_size:]\n",
    "        \n",
    "        X_train = X_train_full[:-val_size]\n",
    "        y_train = y_train_full[:-val_size]\n",
    "        \n",
    "        # One-hot encode labels for training\n",
    "        y_train_enc = one_hot_encode(y_train)\n",
    "        y_val_enc = one_hot_encode(y_val)\n",
    "        y_test_enc = one_hot_encode(y_test)\n",
    "        \n",
    "        print(\"\\nDataset ready for training:\")\n",
    "        print(f\"Training set:   X={X_train.shape}, y={y_train_enc.shape}\")\n",
    "        print(f\"Validation set: X={X_val.shape}, y={y_val_enc.shape}\")\n",
    "        print(f\"Test set:       X={X_test.shape}, y={y_test_enc.shape}\")\n",
    "    else:\n",
    "        print(\"Failed to load dataset.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8637b25f",
   "metadata": {},
   "source": [
    "## 4. Softmax Regression Implementation\n",
    "\n",
    "This is the core component of the project. The class encapsulates the logic derived in Section 2.\n",
    "\n",
    "**Key Implementation Details:**\n",
    "* **Numerical Stability:** We implement `softmax` by subtracting the maximum value from logits to avoid floating-point overflow.\n",
    "* **Mini-batch Gradient Descent:** Instead of updating weights after seeing the whole dataset (Batch GD) or a single example (SGD), we update after a small batch (e.g., 256 samples). This balances speed and stability.\n",
    "* **Vectorization:** We use NumPy matrix operations (dot products) instead of `for` loops wherever possible for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "237072ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    def __init__(self, n_features, n_classes, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initializes the Softmax Regression model parameters.\n",
    "\n",
    "        Args:\n",
    "            n_features (int): Number of input features (e.g., 784 for MNIST).\n",
    "            n_classes (int): Number of output classes (e.g., 10 for MNIST).\n",
    "            learning_rate (float): Step size for Gradient Descent optimization.\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.lr = learning_rate\n",
    "        self.losses = []\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        # W: (n_features, n_classes) - Initialized with small random values\n",
    "        self.W = np.random.randn(n_features, n_classes) * 0.01\n",
    "        \n",
    "        # b: (1, n_classes) - Initialized with zeros\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        Computes the softmax activation function with numerical stability.\n",
    "        Formula: exp(z_i) / sum(exp(z_j))\n",
    "\n",
    "        Args:\n",
    "            z (numpy.ndarray): Linear logits (batch_size, n_classes).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Probabilities (batch_size, n_classes).\n",
    "        \"\"\"\n",
    "        # Subtract max value to prevent overflow (Numerical Stability)\n",
    "        z_stable = z - np.max(z, axis=1, keepdims=True)\n",
    "        exp_z = np.exp(z_stable)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs the forward pass to compute predictions.\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data (batch_size, n_features).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted probabilities.\n",
    "        \"\"\"\n",
    "        # Linear transformation: Z = XW + b\n",
    "        z = np.dot(X, self.W) + self.b\n",
    "        # Activation\n",
    "        return self.softmax(z)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes Cross-Entropy Loss.\n",
    "        L = - sum(y_true * log(y_pred))\n",
    "\n",
    "        Args:\n",
    "            y_true (numpy.ndarray): One-hot encoded ground truth.\n",
    "            y_pred (numpy.ndarray): Predicted probabilities.\n",
    "\n",
    "        Returns:\n",
    "            float: Average loss over the batch.\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        # Add a small epsilon to avoid log(0) error\n",
    "        epsilon = 1e-9\n",
    "        loss = -np.sum(y_true * np.log(y_pred + epsilon)) / m\n",
    "        return loss\n",
    "\n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes gradients of the loss with respect to W and b.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data batch.\n",
    "            y_true (numpy.ndarray): One-hot encoded ground truth.\n",
    "            y_pred (numpy.ndarray): Predicted probabilities.\n",
    "\n",
    "        Returns:\n",
    "            dw, db: Gradients for weights and bias.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Gradient of loss w.r.t Z (logits) is simply (Prediction - Truth)\n",
    "        dz = y_pred - y_true\n",
    "        \n",
    "        # Gradients w.r.t parameters\n",
    "        # dW = (1/m) * X.T . dZ\n",
    "        dw = np.dot(X.T, dz) / m\n",
    "        # db = (1/m) * sum(dZ)\n",
    "        db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return dw, db\n",
    "\n",
    "    def fit(self, X, y, epochs=100, batch_size=256, verbose=True):\n",
    "        \"\"\"\n",
    "        Trains the model using Mini-batch Gradient Descent.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Training features (m, n_features).\n",
    "            y (numpy.ndarray): Training labels One-hot (m, n_classes).\n",
    "            epochs (int): Number of passes over the entire dataset.\n",
    "            batch_size (int): Number of samples per gradient update.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        self.losses = [] # Reset history\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data at the start of each epoch to ensure randomness\n",
    "            indices = np.arange(m)\n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            epoch_loss = 0\n",
    "            num_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "            for i in range(num_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min(start_idx + batch_size, m)\n",
    "                \n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "                # 1. Forward pass\n",
    "                y_pred = self.forward(X_batch)\n",
    "\n",
    "                # 2. Compute Loss (accumulate for reporting)\n",
    "                loss = self.compute_loss(y_batch, y_pred)\n",
    "                epoch_loss += loss * (end_idx - start_idx) \n",
    "\n",
    "                # 3. Backward pass\n",
    "                dw, db = self.backward(X_batch, y_batch, y_pred)\n",
    "\n",
    "                # 4. Update parameters\n",
    "                self.W -= self.lr * dw\n",
    "                self.b -= self.lr * db\n",
    "            \n",
    "            # Average loss for the epoch\n",
    "            avg_loss = epoch_loss / m\n",
    "            self.losses.append(avg_loss)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels for input data.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data (N, n_features).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted class indices (N,).\n",
    "        \"\"\"\n",
    "        y_pred_probs = self.forward(X)\n",
    "        return np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "    def save_weights(self, filepath):\n",
    "        \"\"\"Saves model weights to a .npz file.\"\"\"\n",
    "        # Ensure directory exists\n",
    "        directory = os.path.dirname(filepath)\n",
    "        if directory and not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            \n",
    "        np.savez(filepath, W=self.W, b=self.b)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        \"\"\"Loads model weights from a .npz file.\"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"File not found: {filepath}\")\n",
    "            return\n",
    "            \n",
    "        data = np.load(filepath)\n",
    "        self.W = data['W']\n",
    "        self.b = data['b']\n",
    "        print(f\"Model loaded from {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c6684",
   "metadata": {},
   "source": [
    "## 5. Sanity Check (Overfitting Test)\n",
    "\n",
    "Before training on the full 55,000 images, we perform a **Sanity Check**. \n",
    "We take a tiny subset of real data (e.g., 100 images) and train the model for many epochs. \n",
    "\n",
    "* **Goal:** The model should be able to memorize this small dataset perfectly (Loss $\\to$ 0, Accuracy $\\to$ 100%).\n",
    "* **Result:** If the model fails to overfit this small batch, there is a bug in the code (likely in Gradient Descent or Backpropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dd864cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check subset: (100, 784)\n",
      "\n",
      "Starting Sanity Check...\n",
      "Epoch 10/200 - Loss: 0.3846\n",
      "Epoch 20/200 - Loss: 0.1783\n",
      "Epoch 30/200 - Loss: 0.1117\n",
      "Epoch 40/200 - Loss: 0.0802\n",
      "Epoch 50/200 - Loss: 0.0624\n",
      "Epoch 60/200 - Loss: 0.0512\n",
      "Epoch 70/200 - Loss: 0.0432\n",
      "Epoch 80/200 - Loss: 0.0375\n",
      "Epoch 90/200 - Loss: 0.0330\n",
      "Epoch 100/200 - Loss: 0.0295\n",
      "Epoch 110/200 - Loss: 0.0267\n",
      "Epoch 120/200 - Loss: 0.0244\n",
      "Epoch 130/200 - Loss: 0.0224\n",
      "Epoch 140/200 - Loss: 0.0207\n",
      "Epoch 150/200 - Loss: 0.0193\n",
      "Epoch 160/200 - Loss: 0.0181\n",
      "Epoch 170/200 - Loss: 0.0170\n",
      "Epoch 180/200 - Loss: 0.0160\n",
      "Epoch 190/200 - Loss: 0.0152\n",
      "Epoch 200/200 - Loss: 0.0144\n",
      "\n",
      "Sanity Check Accuracy: 100.00%\n",
      "✅ Sanity Check PASSED: Model logic is correct.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Select a small subset of REAL data\n",
    "subset_size = 100\n",
    "X_subset = X_train[:subset_size]\n",
    "y_subset_enc = y_train_enc[:subset_size]\n",
    "y_subset_labels = y_train[:subset_size] # Original labels for accuracy check\n",
    "\n",
    "print(f\"Sanity check subset: {X_subset.shape}\")\n",
    "\n",
    "# 2. Initialize Model\n",
    "# 784 features (pixels), 10 classes (digits 0-9)\n",
    "model_test = SoftmaxRegression(n_features=784, n_classes=10, learning_rate=0.1)\n",
    "\n",
    "# 3. Train on subset (Overfitting)\n",
    "print(\"\\nStarting Sanity Check...\")\n",
    "model_test.fit(X_subset, y_subset_enc, epochs=200, batch_size=20, verbose=True)\n",
    "\n",
    "# 4. Check Prediction Accuracy\n",
    "preds = model_test.predict(X_subset)\n",
    "acc = np.mean(preds == y_subset_labels)\n",
    "print(f\"\\nSanity Check Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "# 5. Evaluate\n",
    "if acc > 0.95:\n",
    "    print(\"✅ Sanity Check PASSED: Model logic is correct.\")\n",
    "else:\n",
    "    print(\"❌ Sanity Check FAILED: Model cannot learn even small data.\")\n",
    "\n",
    "# 6. Visualize Loss Curve\n",
    "plt.plot(model_test.losses)\n",
    "plt.title(\"Sanity Check Loss Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4845011",
   "metadata": {},
   "source": [
    "### 5.1. Sanity Check Analysis & Discussion\n",
    "\n",
    "**Observation:**\n",
    "As shown in the loss curve above, the training loss decreased significantly from an initial high value to near zero (~0.0144) after 200 epochs. The model achieved **100% accuracy** on the subset of 100 samples.\n",
    "\n",
    "**Discussion on Overfitting:**\n",
    "We observe that the model has successfully **overfitted** this small subset. In the context of a Sanity Check, this is a **positive outcome** because:\n",
    "1.  **Verification of Code Logic:** It confirms that the Forward pass, Backward pass (Gradient calculation), and Update rules are implemented correctly without bugs. If the code were broken, the model would fail to converge even on this tiny dataset.\n",
    "2.  **Model Capacity:** It demonstrates that our Linear Model (Softmax Regression) has sufficient capacity to memorize the mapping of inputs to labels for a small data sample.\n",
    "\n",
    "**Conclusion:**\n",
    "The core implementation is mathematically correct. We can now proceed to training on the full dataset, where our goal will shift from *memorization* (overfitting) to *generalization* (performing well on unseen Test data)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
