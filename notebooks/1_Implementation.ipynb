{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a45fcda",
   "metadata": {},
   "source": [
    "# Lab 02: Softmax Regression - Model Implementation\n",
    "\n",
    "**Group 09:** \n",
    "**Members:**\n",
    "1. Bùi Huy Giáp - 23127289\n",
    "2. Lê Minh Đức - 23127351\n",
    "3. Vũ Tiến Dũng - 23127354\n",
    "4. Đinh Xuân Khương - 23127398\n",
    "5. Nguyễn Đồng Thanh - 23127538 \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4b75e",
   "metadata": {},
   "source": [
    "## 1. Giới thiệu\n",
    "Ở notebook này, nhóm sẽ thực hiện xây dựng mô hình Softmax Regression bằng `Numpy`. Softmax Regression là mô hình khái quát hóa từ Logistic Regression. Theo yêu cầu của đồ án, nhóm sẽ thực hiện huấn luyện và đánh giá mô hình Softmax Regression bằng tập dữ liệu **MNIST**.\n",
    "\n",
    "Trước hết cùng tìm hiểu và nắm rõ các khái niệm, công thức toán học sau:\n",
    "1.  **Hàm điểm tuyến tính (giả thuyết mô hình):** $z = Wx + b$\n",
    "2.  **Hàm Softmax:** Chuyển đổi các điểm số thô thành xác suất.\n",
    "3.  **Hàm mất mát Cross-Entropy:** Đo lường sự khác biệt giữa xác suất dự đoán và nhãn thực tế.\n",
    "4.  **Gradient Descent:** Cập nhật các tham số ($W, b$) để giảm thiểu sai số (mất mát)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb0528",
   "metadata": {},
   "source": [
    "**Note**: Do đây cũng là file ghi chép quá trình nhóm tự học, nên có những dòng ghi chép khá giống độc thoại nội tâm, hoặc tự hỏi tự phản biện, các thầy thông cảm ạ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80451dfa",
   "metadata": {},
   "source": [
    "## 2. Công thức toán học"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f07f5b4",
   "metadata": {},
   "source": [
    "### 2.1. Giả thuyết mô hình (Hàm tính điểm)\n",
    "\n",
    "Nhóm tìm hiểu và thấy hàm chấm điểm mà người ta thường dùng là hàm tuyến tính:\n",
    "\n",
    "$$\n",
    "z = w \\cdot x + b\n",
    "$$\n",
    "\n",
    "**Câu hỏi đặt ra:** *Tại sao lại như vậy? Tại sao $z(x)$ lại là hàm tuyến tính chứ không phải dạng hàm khác?*  \n",
    "\n",
    "Trước hết, softmax là sự tổng quát hóa của logistic, mà logistic cũng dùng hàm chấm điểm tuyến tính. Do đó, nếu thắc mắc thì lẽ ra phải thắc mắc từ lúc học logistic chứ nhỉ !? :> (hi giờ mới để ý nên đào sâu thử)\n",
    "\n",
    "---\n",
    "\n",
    "**Vào trọng tâm: Tại sao hàm chấm điểm lại là *tuyến tính*?**\n",
    "\n",
    "Sau khi tìm hiểu, nhóm rút ra các lý do sau:\n",
    "\n",
    "---\n",
    "\n",
    "**1. Hàm chấm điểm tuyến tính đảm bảo tính tối ưu lồi**\n",
    "\n",
    "- Bài toán tối ưu chỉ có một điểm cực tiểu toàn cục.  \n",
    "- Điều này giúp quá trình học **ổn định, nhanh hơn và dễ phân tích**.  \n",
    "- Nếu dùng hàm phi tuyến cho phần tính điểm, mô hình có thể xuất hiện nhiều cực trị → khó học.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Các trọng số $w_{k,j}$ có thể diễn giải được**\n",
    "\n",
    "- Nhìn vào giá trị của $w_{k,j}$ ta có thể nói lên “tầm ảnh hưởng\" của thuộc tính \\(j\\) lên lớp \\(k\\).  \n",
    "- Điều này quan trọng trong các mô hình cần **tính giải thích**.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Ranh giới quyết định đơn giản**\n",
    "\n",
    "- Khi điểm số là hàm tuyến tính → ranh giới quyết định là:\n",
    "  - 1 đường thẳng (2D),  \n",
    "  - 1 mặt phẳng (3D),  \n",
    "  - hoặc 1 siêu phẳng (nD).\n",
    "\n",
    "- Nhờ đó:\n",
    "  - Dễ quan sát, trực quan hóa và debug.\n",
    "  - Tránh overfit ở một mức nào đó → mô hình học nhanh hơn.\n",
    "\n",
    "---\n",
    "\n",
    "**4. “Xây nhà thì xây từ những thứ cơ bản, đơn giản”**\n",
    "\n",
    "- So với hàm phi tuyến, hàm tuyến tính \\(y = a \\cdot x + b\\) đơn giản hơn và là hàm đơn giản nhất.\n",
    "- Với Softmax Regression, chỉ cần hàm tuyến tính cho phần tính điểm là đủ để đạt:\n",
    "  - độ chính xác tốt,\n",
    "  - tốc độ nhanh,\n",
    "  - cài đặt dễ,\n",
    "  - tốn ít tài nguyên.\n",
    "\n",
    "- Khi cảm thấy hàm tuyến tính không còn đủ chính xác:\n",
    "  - Ta chuyển sang mạng nơ-ron (deep learning),\n",
    "  - nơi hàm tối ưu *không còn lồi*, trọng số khó diễn giải,\n",
    "  - và việc tính toán phức tạp, tốn tài nguyên hơn.\n",
    "\n",
    "Dạng tổng quát:\n",
    "\n",
    "$$\n",
    "\\phi(x) \\rightarrow z = W\\phi(x) + b \\rightarrow \\text{softmax}\n",
    "$$\n",
    "\n",
    "trong đó $\\phi(x)$ là hàm phi tuyến.\n",
    "\n",
    "---\n",
    "\n",
    "### Liên quan: Softmax Regression là một Generalized Linear Model (GLM)\n",
    "\n",
    "- Đây có thể là một lý do khác giải thích tại sao nó sử dụng **hàm chấm điểm tuyến tính** (từ tên đã là *linear* :>).\n",
    "- Phần phi tuyến nằm ở hàm softmax.\n",
    "- Softmax Regression chính là **mạng nơ-ron đơn giản nhất** (1 lớp).\n",
    "- Trong mạng nơ-ron sâu, người ta giải quyết tính phi tuyến bằng cách:\n",
    "  - xếp chồng nhiều lớp tuyến tính,\n",
    "  - xen kẽ bởi các hàm kích hoạt phi tuyến.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cacc5b6",
   "metadata": {},
   "source": [
    "**Đến với công thức tính điểm cho tập MNIST.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a5a47",
   "metadata": {},
   "source": [
    "Với một vector đầu vào $x \\in \\mathbb{R}^{d}$ (trong đó $d$ là số lượng đặc trưng, ví dụ ảnh $28 \\times 28 = 784$ pixel), và với $K$ lớp (ở đây $K = 10$), mô hình tính một điểm tuyến tính (logit) cho mỗi lớp $k$:\n",
    "\n",
    "$$\n",
    "z_k = w_k^T x + b_k\n",
    "$$\n",
    "\n",
    "Ở dạng vector hóa cho một batch gồm $m$ mẫu $X \\in \\mathbb{R}^{m \\times d}$, biểu diễn toán học chính xác là:\n",
    "\n",
    "$$\n",
    "Z = XW + \\mathbf{1}_m b\n",
    "$$\n",
    "\n",
    "Trong đó:\n",
    "\n",
    "- $W \\in \\mathbb{R}^{d \\times K}$ là ma trận trọng số.  \n",
    "- $b \\in \\mathbb{R}^{1 \\times K}$ là vector bias.  \n",
    "- $\\mathbf{1}_m \\in \\mathbb{R}^{m \\times 1}$ là vector cột gồm toàn các số 1.  \n",
    "- $Z \\in \\mathbb{R}^{m \\times K}$ chứa các giá trị logit.\n",
    "\n",
    "Hạng tử $\\mathbf{1}_m b$ thực hiện việc nhân (broadcast) để sao chép vector bias $b$ cho tất cả $m$ mẫu trong batch, tạo thành một ma trận kích thước $(m \\times K)$ để có thể cộng hợp lệ với $XW$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa1dccc",
   "metadata": {},
   "source": [
    "### 2.1.1. Ghi chú về triển khai (Broadcasting)\n",
    "\n",
    "Trong thực tế, khi triển khai bằng các thư viện Python như **NumPy** (hoặc TensorFlow/PyTorch), ta thường bỏ qua vector $\\mathbf{1}_m$ và chỉ viết code (cả viết tài liệu mô tả) dưới dạng:\n",
    "\n",
    "$$Z = XW + b$$\n",
    "\n",
    "**Tại sao lại như vậy?**  \n",
    "Điều này dựa vào cơ chế gọi là **Broadcasting**. Thư viện sẽ tự động nhận diện sự khác biệt kích thước giữa $XW$ $(m \\times K)$ và $b$ $(1 \\times K)$, và ngầm “kéo giãn” (broadcast) vector $b$ theo chiều $m$ để thực hiện phép cộng theo từng phần tử.\n",
    "\n",
    "Do đó, trong code của chúng ta:\n",
    "\n",
    "```python\n",
    "# Code implementation\n",
    "z = np.dot(X, self.W) + self.b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2651aa5d",
   "metadata": {},
   "source": [
    "### 2.2. Hàm Softmax\n",
    "\n",
    "Với một mẫu đơn có vector logit $z = [z_1, z_2, \\dots, z_K]^T$, hàm Softmax sẽ biến đổi các điểm số này thành một phân phối xác suất $\\hat{y}$ (trong đó các phần tử nằm trong khoảng $(0, 1)$ và tổng bằng 1):\n",
    "\n",
    "$$\n",
    "\\hat{y}_k = \\text{Softmax}(z)_k = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "Khi áp dụng cho ma trận logit $Z \\in \\mathbb{R}^{m \\times K}$, phép biến đổi này được thực hiện **theo từng hàng** cho mỗi mẫu.\n",
    "\n",
    "---\n",
    "\n",
    "**Tính ổn định số (Numerical Stability):**  \n",
    "Việc tính trực tiếp $e^{z_k}$ có thể gây tràn số nếu $z_k$ quá lớn. Để tránh điều này, ta sử dụng tính chất bất biến của Softmax khi cộng một hằng số. Ta trừ đi giá trị lớn nhất trong vector $z$ trước khi lấy lũy thừa:\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z)_k = \\frac{e^{z_k - c}}{\\sum_{j=1}^{K} e^{z_j - c}}\n",
    "$$\n",
    "\n",
    "Trong đó $c = \\max(z)$. Điều này đảm bảo số mũ lớn nhất là $0$ ($e^0 = 1$), giúp tránh hiện tượng tràn số.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93238b73",
   "metadata": {},
   "source": [
    "Ở đây, ta sẽ cùng bàn luận hai câu hỏi cơ bản:  \n",
    "\n",
    "---\n",
    "\n",
    "**1. Tại sao cần Softmax?** \n",
    "\n",
    "Ta đã có hàm chấm điểm ở bên trên, nếu muốn phân loại thì chẳng phải chỉ cần lấy nhãn có điểm cao nhất là xong sao?\n",
    "\n",
    "Đúng, nếu muốn phân loại thì chỉ cần lấy điểm cao nhất là xong. Nhưng ta không chỉ muốn phân loại, ta còn muốn:\n",
    "\n",
    "1. **Diễn giải kết quả theo độ tự tin của mô hình**:  \n",
    "   Chỉ xem xét điểm có hai vấn đề:  \n",
    "   - Ta khó lòng thấy bức tranh tổng thể. Ví dụ: điểm cho nhãn A là 100, nhãn B là 60, nhãn C là 75 — ta chỉ thấy con số, nhưng không biết độ tự tin dành cho từng nhãn là bao nhiêu.  \n",
    "   - Nếu lấy “điểm / tổng điểm” để chuẩn hoá thì không hợp lý vì điểm có thể âm.  \n",
    "\n",
    "2. **Tính sai số để đánh giá mô hình**:  \n",
    "   Khi dùng điểm thô, ta không có một thang đo thống nhất.  \n",
    "   Softmax đưa kết quả về khoảng $(0, 1)$ với tổng bằng $1$, cho phép ta dùng các hàm mất mát chuẩn như cross-entropy.  \n",
    "\n",
    "3. **Cải thiện mô hình một cách máy học và tự động**:  \n",
    "   Softmax kết hợp với cross-entropy tạo ra gradient “đẹp”, ổn định:  \n",
    "   $$\n",
    "   \\nabla = \\hat{y} - y\n",
    "   $$\n",
    "   giúp mô hình học hiệu quả và tránh nhiều vấn đề số.\n",
    "\n",
    "4. **Softmax tạo ra một phân phối xác suất hợp lý**:  \n",
    "   Softmax không phải xác suất tuyệt đối theo nghĩa tần suất thống kê, nhưng nó mang đầy đủ đặc tính của xác suất trong mô hình hoá thống kê và hoàn toàn có thể diễn giải như mức độ tự tin.  \n",
    "\n",
    "Tóm lại, Softmax giải quyết được ba nhu cầu quan trọng:  \n",
    "- diễn giải,  \n",
    "- tính loss chuẩn,  \n",
    "- học máy ổn định.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Tại sao lại là $e^x$?**\n",
    "\n",
    "- $e^x$ giúp việc đạo hàm dễ dàng, “soft”, ổn định, và thuận tiện cho tối ưu hoá.  \n",
    "- $e^x$ luôn dương, giúp loại bỏ vấn đề điểm âm khi chuẩn hoá.  \n",
    "- $e^x$ nhạy cảm với sự thay đổi: chênh lệch nhỏ trong điểm thô có thể được “khuếch đại” rõ ràng trong xác suất. Ngoài ra thứ tự vẫn được giữ nguyên vì $e^x$ đồng biến theo x.\n",
    "\n",
    "Nhưng đó chỉ là các lợi ích do ta nhận được sau này.  \n",
    "Nguồn gốc của $e^x$ đến từ Logistic.  \n",
    "Logistic ban đầu xuất phát từ nhu cầu mô hình hóa **log-odds** trong thống kê:\n",
    "\n",
    "$$\n",
    "\\log\\left( \\frac{p}{1-p} \\right) = x\n",
    "$$\n",
    "\n",
    "Suy ra:\n",
    "\n",
    "$$\n",
    "p = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Đây là cách Logistic ra đời.  \n",
    "Softmax chính là tổng quát hoá Logistic cho trường hợp nhiều lớp nên cũng sử dụng $e^x$.\n",
    "\n",
    "---\n",
    "\n",
    "Ngoài các lý do trên, còn có yếu tố quan trọng khác:\n",
    "\n",
    "**Mũ là hàm duy nhất thoả mãn các điều kiện của phân phối Gibbs**\n",
    "\n",
    "Softmax thực chất là phân phối Boltzmann (Gibbs distribution):\n",
    "\n",
    "$$\n",
    "p_k = \\frac{e^{z_k}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "Đây là phân phối duy nhất thoả mãn nhiều tính chất tối ưu liên quan đến entropy trong thống kê và lý thuyết thông tin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2389f616",
   "metadata": {},
   "source": [
    "### 2.3. Hàm mất mát Cross-Entropy\n",
    "\n",
    "Chúng ta tối ưu hóa mô hình bằng cách giảm thiểu hàm mất mát Cross-Entropy. Hàm này đo lường sự khác biệt giữa phân phối xác suất dự đoán $\\hat{y}$ và phân phối nhãn thực tế $y$.\n",
    "\n",
    "#### 2.3.1. Công thức Toán học\n",
    "\n",
    "**1. Đối với một mẫu đơn lẻ (Single Sample):**\n",
    "Giả sử ta có vector nhãn thực tế $y$ (đã one-hot encoding) và vector dự đoán $\\hat{y}$:\n",
    "\n",
    "$$L(y, \\hat{y}) = - \\sum_{k=1}^{K} y_k \\log(\\hat{y}_k)$$\n",
    "\n",
    "Vì $y$ là one-hot (chỉ có một vị trí bằng 1, còn lại bằng 0), công thức rút gọn thực tế là:\n",
    "$$L = - \\log(\\hat{y}_{đúng})$$\n",
    "\n",
    "**2. Đối với cả Batch (m mẫu):**\n",
    "Hàm chi phí (Cost Function) $J$ là trung bình cộng Loss của tất cả $m$ mẫu trong một batch:\n",
    "\n",
    "$$J(W, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y^{(i)}_k \\log(\\hat{y}^{(i)}_k)$$\n",
    "\n",
    "**Rút gọn công thức:**\n",
    "Vì $y^{(i)}$ là vector one-hot, nên tại mỗi mẫu $i$, tổng $\\sum_{k=1}^{K}$ chỉ còn lại duy nhất giá trị của lớp đúng (các lớp khác nhân với 0 nên biến mất). Ta có:\n",
    "\n",
    "$$J(W, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\log(\\hat{y}^{(i)}_{đúng})$$\n",
    "\n",
    "*(Nghĩa là: Duyệt qua từng bức ảnh $i$ trong batch, chỉ lấy log xác suất của **lớp đúng** tương ứng, cộng tất cả lại rồi chia trung bình).*\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.3.2. Ví dụ Tính toán Trực quan\n",
    "\n",
    "Giả sử ta đang huấn luyện batch gồm **3 bức ảnh** ($m=3$) với bài toán phân loại **3 lớp** (Chó, Mèo, Gà).\n",
    "* **Vector $y$ (Thực tế):** [Chó, Mèo, Gà]\n",
    "* **Vector $\\hat{y}$ (Máy dự đoán):** [Chó, Mèo, Gà]\n",
    "\n",
    "| Mẫu (i) | Ảnh thực tế ($y$) | Máy dự đoán ($\\hat{y}$) | Tính toán Loss ($-\\log(\\hat{y}_{đúng})$) | Nhận xét |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **1** | **Chó** $[1, 0, 0]$ | $[0.9, 0.05, 0.05]$ | $-\\log(0.9) \\approx \\mathbf{0.10}$ | Dự đoán đúng & tự tin $\\rightarrow$ **Phạt nhẹ** |\n",
    "| **2** | **Mèo** $[0, 1, 0]$ | $[0.1, 0.3, 0.6]$ | $-\\log(0.3) \\approx \\mathbf{1.20}$ | Dự đoán sai (nghĩ là Gà) $\\rightarrow$ **Phạt nặng** |\n",
    "| **3** | **Gà** $[0, 0, 1]$ | $[0.1, 0.9, 0.0]$ | $-\\log(0.0^*) \\to \\mathbf{+\\infty}$ | Sai tuyệt đối (nghĩ là Mèo) $\\rightarrow$ **Phạt cực đại** |\n",
    "\n",
    "*(**) Trong thực tế code, ta cộng thêm số nhỏ $\\epsilon$ để tránh lỗi log(0).*\n",
    "\n",
    "**Tổng chi phí cho batch này:**\n",
    "$$J = \\frac{0.10 + 1.20 + \\infty}{3} \\approx \\text{Rất lớn (Cần sửa tham số ngay lập tức)}$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.3.3. Phân tích Chi tiết Cross-Entropy\n",
    "\n",
    "**1. Ý nghĩa & Nguồn gốc:**\n",
    "* **Nguồn gốc:** Xuất phát từ **Lý thuyết thông tin (Information Theory)** của Shannon. Khái niệm \"Entropy\" đo lường độ bất ngờ của thông tin.\n",
    "* **Ý nghĩa:** Chúng ta muốn mô hình **không được ngạc nhiên** khi nhìn thấy đáp án đúng.\n",
    "    * Nếu đáp án là \"Chó\" và máy dự đoán 99% là \"Chó\" $\\rightarrow$ Độ ngạc nhiên thấp $\\rightarrow$ Loss thấp.\n",
    "    * Nếu đáp án là \"Chó\" mà máy dự đoán 1% là \"Chó\" $\\rightarrow$ Ngạc nhiên cực độ $\\rightarrow$ Loss cao.\n",
    "\n",
    "**2.Ưu điểm**\n",
    "* **Trừng phạt nặng lỗi \"Ngu dốt mà tự tin\":** Nhìn vào đồ thị hàm $-\\log(x)$, khi xác suất dự đoán sai tiến về 0, giá trị lỗi bắn lên vô cực. Điều này ép mô hình phải sửa sai ngay lập tức, không được phép lơ là các mẫu khó.\n",
    "* **Tương thích hoàn hảo với Softmax:** Khi tính đạo hàm (Gradient Descent), hàm Logarit ($\\log$) triệt tiêu hàm Mũ ($e^x$) của Softmax. Điều này giúp gradient có dạng rất đẹp và đơn giản: $\\hat{y} - y$ (Dự đoán - Thực tế), giúp thuật toán chạy ổn định và nhanh.\n",
    "* **Hàm lồi (Convex):** Đảm bảo thuật toán Gradient Descent luôn tìm được điểm tối ưu toàn cục (global minimum), không bị kẹt ở các điểm tối ưu cục bộ.\n",
    "\n",
    "**3. Nhược điểm:**\n",
    "* **Nhạy cảm với nhiễu (Noise/Outliers):** Vì nó trừng phạt lỗi sai quá nặng, nếu trong dữ liệu huấn luyện có nhãn bị gán sai (ví dụ ảnh con mèo nhưng dán nhãn con chó), mô hình sẽ cố gắng \"học vẹt\" mẫu sai đó một cách cực đoan để giảm Loss, làm giảm độ chính xác tổng quát."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63267050",
   "metadata": {},
   "source": [
    "### 2.4. Gradient Descent\n",
    "\n",
    "Sau khi tính được Loss (sai số), chúng ta cần dùng thuật toán **Gradient Descent** để điều chỉnh bộ trọng số $W$ và bias $b$ sao cho Loss giảm dần về 0. Để làm được điều này, ta cần tính đạo hàm của hàm Loss theo từng tham số.\n",
    "\n",
    "#### 2.4.1. Tính toán Gradient (Đạo hàm)\n",
    "\n",
    "Để thực hiện Gradient Descent, ta áp dụng quy tắc chuỗi (Chain Rule) để tìm đạo hàm của hàm mất mát $L$ theo trọng số $W$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}$$\n",
    "\n",
    "**1. Đạo hàm Loss theo $z$ (Sự kết hợp Cross-Entropy & Softmax):**\n",
    "Mặc dù đạo hàm riêng lẻ của Cross-Entropy ($-\\frac{1}{\\hat{y}}$) và Softmax ($\\hat{y}(1-\\hat{y})$) khá phức tạp, nhưng khi nhân chúng lại với nhau, các thành phần triệt tiêu một cách hoàn hảo:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z} = \\left( -\\frac{1}{\\hat{y}} \\right) \\cdot \\hat{y}(1-\\hat{y}) = \\hat{y} - 1 = \\hat{y} - y$$\n",
    "\n",
    "Đặt $\\delta = \\hat{y} - y$. Đây chính là sai số trực tiếp giữa dự đoán và thực tế.\n",
    "\n",
    "**2. Đạo hàm $z$ theo $W$:**\n",
    "Vì $z = Wx + b$, đạo hàm theo $W$ chính là đầu vào $x$.\n",
    "\n",
    "**3. Kết quả cuối cùng:**\n",
    "$$\\frac{\\partial J}{\\partial W} = \\frac{1}{m} X^T \\cdot \\delta$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum \\delta$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.4.2. Quy luật Cập nhật Tham số (Update Rules)\n",
    "\n",
    "Sau khi đã biết \"hướng dốc\" (gradient), ta thực hiện bước đi xuống dốc để giảm lỗi. Ta dùng một siêu tham số $\\alpha$ (Learning Rate - Tốc độ học) để kiểm soát độ lớn bước đi.\n",
    "\n",
    "$$W := W - \\alpha \\frac{\\partial J}{\\partial W}$$\n",
    "\n",
    "$$b := b - \\alpha \\frac{\\partial J}{\\partial b}$$\n",
    "\n",
    "* Dấu trừ ($-$) nghĩa là đi ngược chiều đạo hàm (đạo hàm tăng thì ta giảm, đạo hàm giảm thì ta tăng) để tiến về điểm cực tiểu.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.4.3. Tổng kết logic trong Code (Implementation Note)\n",
    "\n",
    "Khi viết code Python (Numpy), các công thức trên được viết gọn như sau:\n",
    "\n",
    "```python\n",
    "# 1. Tính sai số (Error term)\n",
    "dz = y_pred - y_true\n",
    "\n",
    "# 2. Tính Gradient (Chain rule)\n",
    "m = X.shape[0]\n",
    "dw = np.dot(X.T, dz) / m\n",
    "db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "\n",
    "# 3. Cập nhật tham số (Gradient Descent step)\n",
    "# self.lr là learning rate (alpha)\n",
    "self.W -= self.lr * dw\n",
    "self.b -= self.lr * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145cbbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import urllib.request # Standard library for downloading files\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7303e5b9",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Helper Functions\n",
    "\n",
    "We need to load the MNIST dataset. To ensure efficiency and portability, we will download the dataset in `.npz` (NumPy Zip) format directly from a reliable source (Google Cloud Storage) if it is not already available locally.\n",
    "\n",
    "We implement the following preprocessing steps:\n",
    "1.  **Loading:** Read `x_train`, `y_train`, `x_test`, `y_test` from the `.npz` file.\n",
    "2.  **Normalization:** Scale pixel intensity values from the range $[0, 255]$ to $[0, 1]$ to ensure numerical stability during gradient descent.\n",
    "3.  **Flattening:** Reshape each $28 \\times 28$ image matrix into a flat feature vector of size $784$ ($28 \\times 28 = 784$). This allows us to perform matrix multiplication with our weight matrix $W$.\n",
    "4.  **One-hot encoding:** Convert integer labels (e.g., $y=5$) into binary vectors (e.g., $[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]$) for Cross-Entropy loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5fe4cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/raw/mnist.npz...\n",
      "Original x_train shape: (60000, 28, 28)\n",
      "Processed x_train shape: (60000, 784)\n",
      "Processed x_test shape: (10000, 784)\n",
      "\n",
      "Dataset ready for training:\n",
      "Training set:   X=(55000, 784), y=(55000, 10)\n",
      "Validation set: X=(5000, 784), y=(5000, 10)\n",
      "Test set:       X=(10000, 784), y=(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "def load_mnist_data(data_path='../data/raw/mnist.npz'):\n",
    "    \"\"\"\n",
    "    Downloads and loads the MNIST dataset from a .npz file.\n",
    "    If the file does not exist, it downloads it from Google Cloud.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to save/load the .npz file.\n",
    "        \n",
    "    Returns:\n",
    "        x_train, y_train, x_test, y_test: Numpy arrays.\n",
    "        - x_train: (60000, 784) - Normalized and Flattened\n",
    "        - y_train: (60000,)     - Raw labels\n",
    "        - x_test:  (10000, 784) - Normalized and Flattened\n",
    "        - y_test:  (10000,)     - Raw labels\n",
    "    \"\"\"\n",
    "    # 1. Download if not exists\n",
    "    url = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\"\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    directory = os.path.dirname(data_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Downloading MNIST data from {url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, data_path)\n",
    "            print(\"Download complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download failed: {e}\")\n",
    "            return None, None, None, None\n",
    "    else:\n",
    "        print(f\"Loading data from {data_path}...\")\n",
    "\n",
    "    # 2. Load data using numpy\n",
    "    with np.load(data_path) as f:\n",
    "        x_train, y_train = f['x_train'], f['y_train']\n",
    "        x_test, y_test = f['x_test'], f['y_test']\n",
    "\n",
    "    print(f\"Original x_train shape: {x_train.shape}\") # Expected: (60000, 28, 28)\n",
    "    \n",
    "    # 3. Preprocessing: Normalize and Flatten\n",
    "    # Scale pixel values to [0, 1] (float32)\n",
    "    x_train = x_train.astype(np.float32) / 255.0\n",
    "    x_test = x_test.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Flatten images: (N, 28, 28) -> (N, 784)\n",
    "    # This is CRITICAL for matrix multiplication: Z = XW + b\n",
    "    x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "    \n",
    "    print(f\"Processed x_train shape: {x_train.shape}\") # Expected: (60000, 784)\n",
    "    print(f\"Processed x_test shape: {x_test.shape}\")   # Expected: (10000, 784)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def one_hot_encode(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts integer labels to one-hot encoded vectors.\n",
    "    e.g., 5 -> [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    \n",
    "    Args:\n",
    "        y (numpy.ndarray): Array of integer labels (m,).\n",
    "        num_classes (int): Number of classes.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: One-hot encoded matrix (m, num_classes).\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    one_hot = np.zeros((m, num_classes))\n",
    "    for i in range(m):\n",
    "        one_hot[i, y[i]] = 1\n",
    "    return one_hot\n",
    "\n",
    "# --- EXECUTE LOADING ---\n",
    "try:\n",
    "    # Load data\n",
    "    X_train_full, y_train_full, X_test, y_test = load_mnist_data()\n",
    "    \n",
    "    if X_train_full is not None:\n",
    "        # Create a Validation set from Train set (last 5000 samples)\n",
    "        # We split to tune hyperparameters without touching the Test set\n",
    "        val_size = 5000\n",
    "        X_val = X_train_full[-val_size:]\n",
    "        y_val = y_train_full[-val_size:]\n",
    "        \n",
    "        X_train = X_train_full[:-val_size]\n",
    "        y_train = y_train_full[:-val_size]\n",
    "        \n",
    "        # One-hot encode labels for training\n",
    "        y_train_enc = one_hot_encode(y_train)\n",
    "        y_val_enc = one_hot_encode(y_val)\n",
    "        y_test_enc = one_hot_encode(y_test)\n",
    "        \n",
    "        print(\"\\nDataset ready for training:\")\n",
    "        print(f\"Training set:   X={X_train.shape}, y={y_train_enc.shape}\")\n",
    "        print(f\"Validation set: X={X_val.shape}, y={y_val_enc.shape}\")\n",
    "        print(f\"Test set:       X={X_test.shape}, y={y_test_enc.shape}\")\n",
    "    else:\n",
    "        print(\"Failed to load dataset.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8637b25f",
   "metadata": {},
   "source": [
    "## 4. Softmax Regression Implementation\n",
    "\n",
    "This is the core component of the project. The class encapsulates the mathematical logic derived in Section 2, enhanced with advanced optimization techniques.\n",
    "\n",
    "**Key Implementation Details:**\n",
    "* **Numerical Stability:** We implement `softmax` by subtracting the maximum value from logits to avoid floating-point overflow ($e^{z_k} \\to \\infty$).\n",
    "* **Vectorization:** We use NumPy matrix operations (dot products) instead of explicit `for` loops for efficient computation.\n",
    "* **Mini-batch Gradient Descent:** We update weights after processing a small batch of samples (e.g., 256) rather than the entire dataset. This balances computational efficiency and convergence stability.\n",
    "* **Momentum:** We incorporate a momentum term ($\\mu$) to accelerate Gradient Descent in the relevant direction and dampen oscillations.\n",
    "* **L2 Regularization:** We add a penalty term $\\frac{\\lambda}{2} ||W||^2$ to the loss function to prevent overfitting and improve generalization on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237072ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SoftmaxRegression:\n",
    "#     def __init__(self, n_features, n_classes, learning_rate=0.01):\n",
    "#         \"\"\"\n",
    "#         Initializes the Softmax Regression model parameters.\n",
    "\n",
    "#         Args:\n",
    "#             n_features (int): Number of input features (e.g., 784 for MNIST).\n",
    "#             n_classes (int): Number of output classes (e.g., 10 for MNIST).\n",
    "#             learning_rate (float): Step size for Gradient Descent optimization.\n",
    "#         \"\"\"\n",
    "#         self.n_features = n_features\n",
    "#         self.n_classes = n_classes\n",
    "#         self.lr = learning_rate\n",
    "#         self.losses = []\n",
    "        \n",
    "#         # Initialize weights and bias\n",
    "#         # W: (n_features, n_classes) - Initialized with small random values\n",
    "#         self.W = np.random.randn(n_features, n_classes) * 0.01\n",
    "        \n",
    "#         # b: (1, n_classes) - Initialized with zeros\n",
    "#         self.b = np.zeros((1, n_classes))\n",
    "\n",
    "#     def softmax(self, z):\n",
    "#         \"\"\"\n",
    "#         Computes the softmax activation function with numerical stability.\n",
    "#         Formula: exp(z_i) / sum(exp(z_j))\n",
    "\n",
    "#         Args:\n",
    "#             z (numpy.ndarray): Linear logits (batch_size, n_classes).\n",
    "\n",
    "#         Returns:\n",
    "#             numpy.ndarray: Probabilities (batch_size, n_classes).\n",
    "#         \"\"\"\n",
    "#         # Subtract max value to prevent overflow (Numerical Stability)\n",
    "#         z_stable = z - np.max(z, axis=1, keepdims=True)\n",
    "#         exp_z = np.exp(z_stable)\n",
    "#         return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         \"\"\"\n",
    "#         Performs the forward pass to compute predictions.\n",
    "        \n",
    "#         Args:\n",
    "#             X (numpy.ndarray): Input data (batch_size, n_features).\n",
    "\n",
    "#         Returns:\n",
    "#             numpy.ndarray: Predicted probabilities.\n",
    "#         \"\"\"\n",
    "#         # Linear transformation: Z = XW + b\n",
    "#         z = np.dot(X, self.W) + self.b\n",
    "#         # Activation\n",
    "#         return self.softmax(z)\n",
    "\n",
    "#     def compute_loss(self, y_true, y_pred):\n",
    "#         \"\"\"\n",
    "#         Computes Cross-Entropy Loss.\n",
    "#         L = - sum(y_true * log(y_pred))\n",
    "\n",
    "#         Args:\n",
    "#             y_true (numpy.ndarray): One-hot encoded ground truth.\n",
    "#             y_pred (numpy.ndarray): Predicted probabilities.\n",
    "\n",
    "#         Returns:\n",
    "#             float: Average loss over the batch.\n",
    "#         \"\"\"\n",
    "#         m = y_true.shape[0]\n",
    "#         # Add a small epsilon to avoid log(0) error\n",
    "#         epsilon = 1e-9\n",
    "#         loss = -np.sum(y_true * np.log(y_pred + epsilon)) / m\n",
    "#         return loss\n",
    "\n",
    "#     def backward(self, X, y_true, y_pred):\n",
    "#         \"\"\"\n",
    "#         Computes gradients of the loss with respect to W and b.\n",
    "\n",
    "#         Args:\n",
    "#             X (numpy.ndarray): Input data batch.\n",
    "#             y_true (numpy.ndarray): One-hot encoded ground truth.\n",
    "#             y_pred (numpy.ndarray): Predicted probabilities.\n",
    "\n",
    "#         Returns:\n",
    "#             dw, db: Gradients for weights and bias.\n",
    "#         \"\"\"\n",
    "#         m = X.shape[0]\n",
    "        \n",
    "#         # Gradient of loss w.r.t Z (logits) is simply (Prediction - Truth)\n",
    "#         dz = y_pred - y_true\n",
    "        \n",
    "#         # Gradients w.r.t parameters\n",
    "#         # dW = (1/m) * X.T . dZ\n",
    "#         dw = np.dot(X.T, dz) / m\n",
    "#         # db = (1/m) * sum(dZ)\n",
    "#         db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "        \n",
    "#         return dw, db\n",
    "\n",
    "#     def fit(self, X, y, epochs=100, batch_size=256, verbose=True):\n",
    "#         \"\"\"\n",
    "#         Trains the model using Mini-batch Gradient Descent.\n",
    "\n",
    "#         Args:\n",
    "#             X (numpy.ndarray): Training features (m, n_features).\n",
    "#             y (numpy.ndarray): Training labels One-hot (m, n_classes).\n",
    "#             epochs (int): Number of passes over the entire dataset.\n",
    "#             batch_size (int): Number of samples per gradient update.\n",
    "#         \"\"\"\n",
    "#         m = X.shape[0]\n",
    "#         self.losses = [] # Reset history\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             # Shuffle data at the start of each epoch to ensure randomness\n",
    "#             indices = np.arange(m)\n",
    "#             np.random.shuffle(indices)\n",
    "#             X_shuffled = X[indices]\n",
    "#             y_shuffled = y[indices]\n",
    "\n",
    "#             epoch_loss = 0\n",
    "#             num_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "#             for i in range(num_batches):\n",
    "#                 start_idx = i * batch_size\n",
    "#                 end_idx = min(start_idx + batch_size, m)\n",
    "                \n",
    "#                 X_batch = X_shuffled[start_idx:end_idx]\n",
    "#                 y_batch = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "#                 # 1. Forward pass\n",
    "#                 y_pred = self.forward(X_batch)\n",
    "\n",
    "#                 # 2. Compute Loss (accumulate for reporting)\n",
    "#                 loss = self.compute_loss(y_batch, y_pred)\n",
    "#                 epoch_loss += loss * (end_idx - start_idx) \n",
    "\n",
    "#                 # 3. Backward pass\n",
    "#                 dw, db = self.backward(X_batch, y_batch, y_pred)\n",
    "\n",
    "#                 # 4. Update parameters\n",
    "#                 self.W -= self.lr * dw\n",
    "#                 self.b -= self.lr * db\n",
    "            \n",
    "#             # Average loss for the epoch\n",
    "#             avg_loss = epoch_loss / m\n",
    "#             self.losses.append(avg_loss)\n",
    "            \n",
    "#             if verbose and (epoch + 1) % 10 == 0:\n",
    "#                 print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"\n",
    "#         Predicts class labels for input data.\n",
    "\n",
    "#         Args:\n",
    "#             X (numpy.ndarray): Input data (N, n_features).\n",
    "\n",
    "#         Returns:\n",
    "#             numpy.ndarray: Predicted class indices (N,).\n",
    "#         \"\"\"\n",
    "#         y_pred_probs = self.forward(X)\n",
    "#         return np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "#     def save_weights(self, filepath):\n",
    "#         \"\"\"Saves model weights to a .npz file.\"\"\"\n",
    "#         # Ensure directory exists\n",
    "#         directory = os.path.dirname(filepath)\n",
    "#         if directory and not os.path.exists(directory):\n",
    "#             os.makedirs(directory)\n",
    "            \n",
    "#         np.savez(filepath, W=self.W, b=self.b)\n",
    "#         print(f\"Model saved to {filepath}\")\n",
    "\n",
    "#     def load_weights(self, filepath):\n",
    "#         \"\"\"Loads model weights from a .npz file.\"\"\"\n",
    "#         if not os.path.exists(filepath):\n",
    "#             print(f\"File not found: {filepath}\")\n",
    "#             return\n",
    "            \n",
    "#         data = np.load(filepath)\n",
    "#         self.W = data['W']\n",
    "#         self.b = data['b']\n",
    "#         print(f\"Model loaded from {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df8c8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    def __init__(self, n_features, n_classes, learning_rate=0.1, momentum=0.9, l2_reg=1e-4):\n",
    "        \"\"\"\n",
    "        Softmax Regression with Momentum and L2 Regularization.\n",
    "\n",
    "        Args:\n",
    "            n_features (int): Number of input features (e.g., 784).\n",
    "            n_classes (int): Number of output classes (e.g., 10).\n",
    "            learning_rate (float): Step size for parameter updates.\n",
    "            momentum (float): Momentum factor (0.0 to 1.0) to accelerate convergence.\n",
    "            l2_reg (float): L2 regularization strength (lambda) to prevent overfitting.\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.l2_reg = l2_reg\n",
    "        self.losses = []\n",
    "        \n",
    "        # Initialize weights (W) and bias (b)\n",
    "        # Using Xavier/He initialization concept (small random numbers)\n",
    "        self.W = np.random.randn(n_features, n_classes) * 0.01\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "        \n",
    "        # Velocity terms for Momentum\n",
    "        self.v_W = np.zeros_like(self.W)\n",
    "        self.v_b = np.zeros_like(self.b)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        Computes the stable softmax of vector z.\n",
    "        Args:\n",
    "            z (numpy.ndarray): Logits (batch_size, n_classes).\n",
    "        Returns:\n",
    "            numpy.ndarray: Probabilities summing to 1 for each row.\n",
    "        \"\"\"\n",
    "        # Numerical stability: subtract max(z) to prevent exponential overflow\n",
    "        z_stable = z - np.max(z, axis=1, keepdims=True)\n",
    "        exp_z = np.exp(z_stable)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs forward pass: Z = XW + b -> Softmax(Z).\n",
    "        \"\"\"\n",
    "        z = np.dot(X, self.W) + self.b\n",
    "        return self.softmax(z)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes Cross-Entropy Loss with L2 Regularization penalty.\n",
    "        Cost = -Sum(y * log(y_hat)) + (lambda/2) * ||W||^2\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        epsilon = 1e-9 # Prevent log(0)\n",
    "        \n",
    "        # Standard Cross-Entropy\n",
    "        data_loss = -np.sum(y_true * np.log(y_pred + epsilon)) / m\n",
    "        \n",
    "        # L2 Regularization Term\n",
    "        reg_loss = (self.l2_reg / 2) * np.sum(np.square(self.W))\n",
    "        \n",
    "        return data_loss + reg_loss\n",
    "\n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes gradients with respect to W and b (including L2 term).\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        dz = y_pred - y_true\n",
    "        \n",
    "        # Gradient of W: (1/m) * X.T.dot(dZ) + lambda * W\n",
    "        dw = (np.dot(X.T, dz) / m) + (self.l2_reg * self.W)\n",
    "        \n",
    "        # Gradient of b: (1/m) * sum(dZ)\n",
    "        db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return dw, db\n",
    "\n",
    "    def fit(self, X, y, epochs=100, batch_size=256, verbose=True):\n",
    "        \"\"\"\n",
    "        Trains the model using Mini-batch Gradient Descent with Momentum.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        self.losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.arange(m)\n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            epoch_loss = 0\n",
    "            num_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "            for i in range(num_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min(start_idx + batch_size, m)\n",
    "                \n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "                # 1. Forward\n",
    "                y_pred = self.forward(X_batch)\n",
    "\n",
    "                # 2. Loss\n",
    "                loss = self.compute_loss(y_batch, y_pred)\n",
    "                epoch_loss += loss * (end_idx - start_idx)\n",
    "\n",
    "                # 3. Backward\n",
    "                dw, db = self.backward(X_batch, y_batch, y_pred)\n",
    "\n",
    "                # 4. Update with Momentum\n",
    "                # v = momentum * v - learning_rate * gradient\n",
    "                # W = W + v\n",
    "                # Note: We implement standard SGD update logic: W = W - (lr*grad + mom*v_prev)\n",
    "                # Here uses simple Polyak Momentum\n",
    "                self.v_W = self.momentum * self.v_W + self.lr * dw\n",
    "                self.v_b = self.momentum * self.v_b + self.lr * db\n",
    "                \n",
    "                self.W -= self.v_W\n",
    "                self.b -= self.v_b\n",
    "            \n",
    "            avg_loss = epoch_loss / m\n",
    "            self.losses.append(avg_loss)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Returns class indices with highest probability.\"\"\"\n",
    "        y_pred_probs = self.forward(X)\n",
    "        return np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "    def save_weights(self, filepath):\n",
    "        \"\"\"Saves W and b to .npz file.\"\"\"\n",
    "        directory = os.path.dirname(filepath)\n",
    "        if directory and not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        np.savez(filepath, W=self.W, b=self.b)\n",
    "        print(f\"Weights saved to {filepath}\")\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        \"\"\"Loads W and b from .npz file.\"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"File {filepath} not found.\")\n",
    "            return False\n",
    "        data = np.load(filepath)\n",
    "        self.W = data['W']\n",
    "        self.b = data['b']\n",
    "        print(f\"Weights loaded from {filepath}\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c6684",
   "metadata": {},
   "source": [
    "## 5. Sanity Check (Overfitting Test)\n",
    "\n",
    "Before training on the full 55,000 images, we perform a **Sanity Check**. \n",
    "We take a tiny subset of real data (e.g., 100 images) and train the model for many epochs. \n",
    "\n",
    "* **Goal:** The model should be able to memorize this small dataset perfectly (Loss $\\to$ 0, Accuracy $\\to$ 100%).\n",
    "* **Result:** If the model fails to overfit this small batch, there is a bug in the code (likely in Gradient Descent or Backpropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dd864cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check subset: (100, 784)\n",
      "\n",
      "Starting Sanity Check...\n",
      "Epoch 10/200 - Loss: 0.0114\n",
      "Epoch 20/200 - Loss: 0.0086\n",
      "Epoch 30/200 - Loss: 0.0080\n",
      "Epoch 40/200 - Loss: 0.0075\n",
      "Epoch 50/200 - Loss: 0.0072\n",
      "Epoch 60/200 - Loss: 0.0070\n",
      "Epoch 70/200 - Loss: 0.0068\n",
      "Epoch 80/200 - Loss: 0.0067\n",
      "Epoch 90/200 - Loss: 0.0066\n",
      "Epoch 100/200 - Loss: 0.0065\n",
      "Epoch 110/200 - Loss: 0.0064\n",
      "Epoch 120/200 - Loss: 0.0063\n",
      "Epoch 130/200 - Loss: 0.0063\n",
      "Epoch 140/200 - Loss: 0.0062\n",
      "Epoch 150/200 - Loss: 0.0062\n",
      "Epoch 160/200 - Loss: 0.0061\n",
      "Epoch 170/200 - Loss: 0.0061\n",
      "Epoch 180/200 - Loss: 0.0061\n",
      "Epoch 190/200 - Loss: 0.0061\n",
      "Epoch 200/200 - Loss: 0.0060\n",
      "\n",
      "Sanity Check Accuracy: 100.00%\n",
      "✅ Sanity Check PASSED: Model logic is correct.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Select a small subset of REAL data\n",
    "subset_size = 100\n",
    "X_subset = X_train[:subset_size]\n",
    "y_subset_enc = y_train_enc[:subset_size]\n",
    "y_subset_labels = y_train[:subset_size] # Original labels for accuracy check\n",
    "\n",
    "print(f\"Sanity check subset: {X_subset.shape}\")\n",
    "\n",
    "# 2. Initialize Model\n",
    "# 784 features (pixels), 10 classes (digits 0-9)\n",
    "model_test = SoftmaxRegression(n_features=784, n_classes=10, learning_rate=0.1)\n",
    "\n",
    "# 3. Train on subset (Overfitting)\n",
    "print(\"\\nStarting Sanity Check...\")\n",
    "model_test.fit(X_subset, y_subset_enc, epochs=200, batch_size=20, verbose=True)\n",
    "\n",
    "# 4. Check Prediction Accuracy\n",
    "preds = model_test.predict(X_subset)\n",
    "acc = np.mean(preds == y_subset_labels)\n",
    "print(f\"\\nSanity Check Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "# 5. Evaluate\n",
    "if acc > 0.95:\n",
    "    print(\"✅ Sanity Check PASSED: Model logic is correct.\")\n",
    "else:\n",
    "    print(\"❌ Sanity Check FAILED: Model cannot learn even small data.\")\n",
    "\n",
    "# 6. Visualize Loss Curve\n",
    "plt.plot(model_test.losses)\n",
    "plt.title(\"Sanity Check Loss Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4845011",
   "metadata": {},
   "source": [
    "### 5.1. Sanity Check Analysis & Discussion\n",
    "\n",
    "**Observation:**\n",
    "As shown in the loss curve above, the training loss decreased significantly from an initial high value to near zero. The model achieved **100% accuracy** (or very close to it) on the subset of 100 samples.\n",
    "\n",
    "**Discussion on Overfitting:**\n",
    "We observe that the model has successfully **overfitted** this small subset. In the context of a Sanity Check, this is a **positive outcome** because:\n",
    "1.  **Verification of Code Logic:** It confirms that the Forward pass, Backward pass (Gradient calculation), and Update rules are implemented correctly without bugs. If the code were broken, the model would fail to converge even on this tiny dataset.\n",
    "2.  **Model Capacity:** It demonstrates that our Linear Model (Softmax Regression) has sufficient capacity to memorize the mapping of inputs to labels for a small data sample.\n",
    "\n",
    "**Conclusion:**\n",
    "The core implementation is mathematically correct. We can now proceed to training on the full dataset, where our goal will shift from *memorization* (overfitting) to *generalization* (performing well on unseen Test data)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
